<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Week 4 #  Topic: The Self-Distillation Family in Self-Supervised Learning
Keynote Speaker: Lifan Lin, Yue Wu, Shengjie Niu
Time: Jul 13, 19:30 - 21:30 pm
Venue: Lecture Hall 3, 302 (SUSTech)
Online Link: TencentMeeting
Compendium #  I. Introduction: Concept and Mechanism of Self-Distillation
 Introduce the basic concept and structure of self-distillation. Discuss the mechanisms of self-distillation and its relation with Knowledge distillation. Discuss the basic idea of contrastive learning and its pretext task (SimCLR)."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="Week 4 #  Topic: The Self-Distillation Family in Self-Supervised Learning
Keynote Speaker: Lifan Lin, Yue Wu, Shengjie Niu
Time: Jul 13, 19:30 - 21:30 pm
Venue: Lecture Hall 3, 302 (SUSTech)
Online Link: TencentMeeting
Compendium #  I. Introduction: Concept and Mechanism of Self-Distillation
 Introduce the basic concept and structure of self-distillation. Discuss the mechanisms of self-distillation and its relation with Knowledge distillation. Discuss the basic idea of contrastive learning and its pretext task (SimCLR)."><meta property="og:type" content="article"><meta property="og:url" content="https://niusj03.github.io/23summer/docs/topics/week4/"><meta property="article:section" content="docs"><title>Week4 | 23 Summer Study</title><link rel=manifest href=/23summer/manifest.json><link rel=icon href=/23summer/favicon.png type=image/x-icon><link rel=stylesheet href=/23summer/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz+OQteg=" crossorigin=anonymous><script defer src=/23summer/flexsearch.min.js></script>
<script defer src=/23summer/en.search.min.d8921074c9209c7ad6a05de7acf3f7efc197fb4be143e560c149c7d4e9619663.js integrity="sha256-2JIQdMkgnHrWoF3nrPP378GX+0vhQ+VgwUnH1OlhlmM=" crossorigin=anonymous></script>
<script defer src=/23summer/sw.min.e333349655ab7baad14b3d2ed0333503afc9a1297b9c4c9bc81b88a385502a47.js integrity="sha256-4zM0llWre6rRSz0u0DM1A6/JoSl7nEybyBuIo4VQKkc=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/23summer/><span>23 Summer Study</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/23summer/docs/acknowledge/>Acknowledgement</a></li><li><a href=/23summer/docs/introduction/>Introduction</a></li><li><a href=/23summer/docs/schedule/>Schedule</a></li><li class=book-section-flat><span>Topics</span><ul><li><a href=/23summer/docs/topics/week1/>Week1</a></li><li><a href=/23summer/docs/topics/week2/>Week2</a></li><li><a href=/23summer/docs/topics/week3/>Week3</a></li><li><a href=/23summer/docs/topics/week4/ class=active>Week4</a></li><li><a href=/23summer/docs/topics/week5/>Week5</a></li><li><a href=/23summer/docs/topics/week6/>Week6</a></li><li><a href=/23summer/docs/topics/week7/>Week7</a></li><li><a href=/23summer/docs/topics/week8/>Week8</a></li></ul></li></ul><ul><li><a href=https://github.com/niusj03/23summer target=_blank rel=noopener>Github</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/23summer/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Week4</strong>
<label for=toc-control><img src=/23summer/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#compendium>Compendium</a></li><li><a href=#material>Material</a></li><li><a href=#references>References</a></li></ul></nav></aside></header><article class=markdown><h1 id=week-4>Week 4
<a class=anchor href=#week-4>#</a></h1><p>Topic: The Self-Distillation Family in Self-Supervised Learning</p><p>Keynote Speaker: Lifan Lin, Yue Wu, Shengjie Niu</p><p>Time: Jul 13, 19:30 - 21:30 pm</p><p>Venue: Lecture Hall 3, 302 (SUSTech)</p><p>Online Link:
<a href=https://sustech.meeting.tencent.com/dm/rzsV1UdvWHtp>TencentMeeting</a></p><h2 id=compendium>Compendium
<a class=anchor href=#compendium>#</a></h2><p>I. Introduction: Concept and Mechanism of Self-Distillation</p><ul><li>Introduce the basic concept and structure of self-distillation. Discuss the mechanisms of self-distillation and its relation with Knowledge distillation.</li><li>Discuss the basic idea of contrastive learning and its pretext task (SimCLR).</li><li>The learning objective: A representation mapping invariant of transformation(augmentation). Alignment and uniformity are two key requirement.</li><li>Collapse(trivial solution). Optimal but unwanted result. Basic concept in preventing collapse.</li></ul><p>II. Contrastive Learning without Negative Samples</p><ul><li>Introduce BYOL, a self-distillation model learning without negative pairs.</li><li>Connections between BYOL and other works</li><li>Provide some illuminations about why BYOL performs well and avoids collapse</li></ul><p>III. Examination of BYOL-like model: Road to prevent collapse</p><ul><li>Mythology: Studying the components of the model through ablation to understand whether they are necessary in preventing collapse.</li><li>Hypothesis proposed: Procedures taken are actually solving an underlying optimization problem. The optimization is EM-like and thus do well in searching for a representation.</li><li>Validating the hypothesis vis experience.</li></ul><p>IV. Combining Transformer with Self-Distillation</p><ul><li>Discuss the difficulty faced by the ViT(Vision Transformer). Specifically, the tokenizer of images.</li><li>Tokenizer learn better deep semantic information using self-distillation.</li><li>Discussing the trade-off between alignment and uniformity in ViT.</li><li>Improvement: Introduction MIM(Masked Image Modelling, much similar to masked language modelling) to self-distillation to create more effective pretext tasks.</li></ul><h2 id=material>Material
<a class=anchor href=#material>#</a></h2><p>I.
<a href=https://nbviewer.org/github/niusj03/23summer/blob/master/content/docs/pdfs/Week4_Niu.pdf>Slide</a> for Intro to Self-Supervised Learning from Shengjie Niu.</p><p>II. Slides
<a href=https://nbviewer.org/github/niusj03/23summer/blob/master/content/docs/pdfs/Week4_Wu.pdf>1</a> and
<a href=https://nbviewer.org/github/niusj03/23summer/blob/master/content/docs/pdfs/Week4_Lin.pdf>2</a> for Self-Distillation Family from Yue Wu and Lifan Lin.</p><h2 id=references>References
<a class=anchor href=#references>#</a></h2><ol><li><p>Balestriero, R et al.
<a href=https://arxiv.org/abs/2304.12210>A Cookbook of Self-supervised Learning</a></p></li><li><p><a href=https://arxiv.org/abs/2006.07733>Bootstrap your own latent: A new approach to self-supervised Learning</a></p></li><li><p><a href=https://arxiv.org/abs/2011.10566>Exploring Simple Siamese Representation Learning</a></p></li><li><p><a href=https://arxiv.org/abs/2104.14294>Emerging Properties in Self-Supervised Vision Transformers</a></p></li><li><p><a href=https://generallyintelligent.com/research/2020-08-24-understanding-self-supervised-contrastive-learning/>A blog about BYOL</a></p></li><li><p><a href=https://arxiv.org/abs/2002.05709>A Simple Framework for Contrastive Learning of Visual Representations (arxiv.org)</a></p></li><li><p><a href=https://arxiv.org/abs/2006.07733>Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)</a></p></li><li><p><a href=https://arxiv.org/abs/2011.10566>Exploring Simple Siamese Representation Learning (arxiv.org)</a></p></li><li><p><a href=https://arxiv.org/abs/2104.14294>Emerging Properties in Self-Supervised Vision Transformers (arxiv.org)</a></p></li><li><p><a href=https://arxiv.org/abs/2111.07832>iBOT: Image BERT Pre-Training with Online Tokenizer (arxiv.org)</a></p></li><li><p><a href=https://drscotthawley.github.io/blog_quarto/posts/2022-11-17-byol.html>blog_quarto - BYOL: Contrastive Representation-Learning without Contrastive Losses (drscotthawley.github.io)</a></p></li><li><p><a href=https://generallyintelligent.com/research/2020-08-24-understanding-self-supervised-contrastive-learning/>Understanding self-supervised and contrastive learning with &ldquo;Bootstrap Your Own Latent&rdquo; (BYOL) - generally intelligent</a></p></li></ol></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(n){const e=window.getSelection(),t=document.createRange();t.selectNodeContents(n),e.removeAllRanges(),e.addRange(t)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#compendium>Compendium</a></li><li><a href=#material>Material</a></li><li><a href=#references>References</a></li></ul></nav></div></aside></main></body></html>