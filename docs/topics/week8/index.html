<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Week 8 #  Topic: The simple summary of Self-Supervised Learning & Masked Image Model in Self-Supervised Learning Family in Self-Supervised Learning
Keynote Speaker: Shengjie Niu, Zebin Yun
Time: Aug 10, 19:30 - 21:30 pm
Venue: Lecture Hall 3, 302 (SUSTech)
Online Link: TencentMeeting
Compendium #  I. Stage-1: Verstile developments among the field of SSL
 InstDisc proposed new pretext task of instance-level discrimination. CPC employed the predictive pretxt task on SSL, so as to realize wide applicability."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="Week 8 #  Topic: The simple summary of Self-Supervised Learning & Masked Image Model in Self-Supervised Learning Family in Self-Supervised Learning
Keynote Speaker: Shengjie Niu, Zebin Yun
Time: Aug 10, 19:30 - 21:30 pm
Venue: Lecture Hall 3, 302 (SUSTech)
Online Link: TencentMeeting
Compendium #  I. Stage-1: Verstile developments among the field of SSL
 InstDisc proposed new pretext task of instance-level discrimination. CPC employed the predictive pretxt task on SSL, so as to realize wide applicability."><meta property="og:type" content="article"><meta property="og:url" content="https://niusj03.github.io/23summer/docs/topics/week8/"><meta property="article:section" content="docs"><title>Week8 | 23 Summer Study</title><link rel=manifest href=/23summer/manifest.json><link rel=icon href=/23summer/favicon.png type=image/x-icon><link rel=stylesheet href=/23summer/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz+OQteg=" crossorigin=anonymous><script defer src=/23summer/flexsearch.min.js></script>
<script defer src=/23summer/en.search.min.d8921074c9209c7ad6a05de7acf3f7efc197fb4be143e560c149c7d4e9619663.js integrity="sha256-2JIQdMkgnHrWoF3nrPP378GX+0vhQ+VgwUnH1OlhlmM=" crossorigin=anonymous></script>
<script defer src=/23summer/sw.min.e333349655ab7baad14b3d2ed0333503afc9a1297b9c4c9bc81b88a385502a47.js integrity="sha256-4zM0llWre6rRSz0u0DM1A6/JoSl7nEybyBuIo4VQKkc=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/23summer/><span>23 Summer Study</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/23summer/docs/acknowledge/>Acknowledgement</a></li><li><a href=/23summer/docs/introduction/>Introduction</a></li><li><a href=/23summer/docs/schedule/>Schedule</a></li><li class=book-section-flat><span>Topics</span><ul><li><a href=/23summer/docs/topics/week1/>Week1</a></li><li><a href=/23summer/docs/topics/week2/>Week2</a></li><li><a href=/23summer/docs/topics/week3/>Week3</a></li><li><a href=/23summer/docs/topics/week4/>Week4</a></li><li><a href=/23summer/docs/topics/week5/>Week5</a></li><li><a href=/23summer/docs/topics/week6/>Week6</a></li><li><a href=/23summer/docs/topics/week7/>Week7</a></li><li><a href=/23summer/docs/topics/week8/ class=active>Week8</a></li></ul></li></ul><ul><li><a href=https://github.com/niusj03/23summer target=_blank rel=noopener>Github</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/23summer/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Week8</strong>
<label for=toc-control><img src=/23summer/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#compendium>Compendium</a></li><li><a href=#material>Material</a></li><li><a href=#reference>Reference</a></li></ul></nav></aside></header><article class=markdown><h1 id=week-8>Week 8
<a class=anchor href=#week-8>#</a></h1><p>Topic: The simple summary of Self-Supervised Learning & Masked Image Model in Self-Supervised Learning Family in Self-Supervised Learning</p><p>Keynote Speaker: Shengjie Niu, Zebin Yun</p><p>Time: Aug 10, 19:30 - 21:30 pm</p><p>Venue: Lecture Hall 3, 302 (SUSTech)</p><p>Online Link:
<a href=https://meeting.tencent.com/dm/ciI4lpALLhxz>TencentMeeting</a></p><h2 id=compendium>Compendium
<a class=anchor href=#compendium>#</a></h2><p>I. Stage-1: Verstile developments among the field of SSL</p><ul><li>InstDisc proposed new pretext task of instance-level discrimination.</li><li>CPC employed the predictive pretxt task on SSL, so as to realize wide applicability.</li><li>InvaSpread utilize the negative pairs among the batch samples.</li><li>CMC first proposed the utilization from the perspective of mutiview.</li><li>Stage conclusion in terms of objective functions, pretext tasks and construction of negative pairs.</li></ul><p>II. Stage-2: Two distinguished methodologies lead the trend</p><ul><li>MoCo proposed momentum encoder and query dictionary for contrastive Self-Supervised Learning.</li><li>SimCLR proposed a simple but effective framework to claim the function of augmentation strategies and project head.</li><li>MoCo v2 and SimCLR v2</li><li>SeLa first compared the instance with clustering prototypes instead of negative pairs, and propose a new component of predict head.</li><li>SwAV achieve the promising performance with sway mode of SeLa, leading to the following trend of learning without negative pairs.</li></ul><p>III. Stage-3: Learning without negative pairs</p><ul><li>BYOL: first Self-Supervised Learning literature finished its training totally without negative pairs.</li><li>SimSiam summarized and analysed the function of all kinds of components, resulting in a structure-easy but effective model.</li></ul><p><strong>Significant Timepoint: (2020.10) Vision Transfermer(ViT) brought large impacts on computer vision field and promoted the progress of generative vision model based on ViT.</strong></p><p>IV. Masked Image Model (MIM) family in Self-Supervised Learning</p><ul><li>BEiT trained transformer-based model to reconstruct the input in the form of token-level label (In need of pre-trained tokenizer)</li><li>MAE trained transformer-based model to reconstercut the input in pixel level</li><li>SimMIM summarized some MIM models and found a series of simple but effective components.</li></ul><h2 id=material>Material
<a class=anchor href=#material>#</a></h2><p><a href=https://www.overleaf.com/read/dshmrcjjvyfk>Slides & Source Code</a> for Week-8 seminar from Shengjie Niu.</p><p><a href=https://nbviewer.org/github/niusj03/23summer/blob/master/content/docs/pdfs/BigP.pdf>Big Picture</a> from Shengjie Niu.</p><h2 id=reference>Reference
<a class=anchor href=#reference>#</a></h2><ol><li><p>Z. Wu et al,
<a href=https://arxiv.org/pdf/1805.01978.pdf>Unsupervised Feature Learning via Non-Parametric Instance Discrimination</a>.</p></li><li><p>A. Oord et al,
<a href=https://arxiv.org/abs/1807.03748>Representation Learning with Contrastive Predictive Coding</a>.</p></li><li><p>M. Ye et al,
<a href=https://arxiv.org/abs/1904.03436>Unsupervised Embedding Learning via Invariant and Spreading Instance Feature</a>.</p></li><li><p>Y. Tian et al,
<a href=https://arxiv.org/abs/1906.05849>Contrastive Multiview Coding</a>.</p></li><li><p>K. He et al,
<a href=https://arxiv.org/abs/1911.05722>Momentum Contrast for Unsupervised Visual Representation Learning</a>.</p></li><li><p>T. Chen et al,
<a href=https://arxiv.org/abs/2002.05709>A Simple Framework for Contrastive Learning of Visual Representations</a>.</p></li><li><p>X. Chen et al,
<a href=https://arxiv.org/abs/2003.04297>Improved Baselines with Momentum Contrastive Learning</a>.</p></li><li><p>T. Chen et al,
<a href=https://arxiv.org/abs/2006.10029>Big Self-Supervised Models are Strong Semi-Supervised Learners</a>.</p></li><li><p>Y. Asano et al,
<a href=https://arxiv.org/abs/1911.05371>Self-labelling via simultaneous clustering and representation learning</a>.</p></li><li><p>M. Caron et al,
<a href=https://arxiv.org/abs/2006.09882>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</a>.</p></li><li><p>J. Grill et al,
<a href=https://arxiv.org/abs/2006.07733>Bootstrap your own latent: A new approach to self-supervised Learning</a>.</p></li><li><p>X. Chen et al,
<a href=https://arxiv.org/abs/2011.10566>Exploring Simple Siamese Representation Learning</a>.</p></li><li><p>M. Caron et al,
<a href=https://arxiv.org/abs/2104.14294>Emerging Properties in Self-Supervised Vision Transformers</a>.</p></li><li><p>B. Hao et al,
<a href=https://arxiv.org/abs/2106.08254>BEiT: BERT Pre-Training of Image Transformers</a>.</p></li><li><p>K He et al,
<a href=https://arxiv.org/abs/2111.06377>Masked Autoencoders Are Scalable Vision Learners</a>.</p></li><li><p>Z Xie et al,
<a href=https://arxiv.org/abs/2111.09886>SimMIM: A Simple Framework for Masked Image Modeling</a>.</p></li></ol></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(n){const e=window.getSelection(),t=document.createRange();t.selectNodeContents(n),e.removeAllRanges(),e.addRange(t)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#compendium>Compendium</a></li><li><a href=#material>Material</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></aside></main></body></html>