[{"id":0,"href":"/23summer/docs/acknowledge/","title":"Acknowledgement","section":"Docs","content":"Acknowledgement #  Eight weeks have flown by, and as our self-supervised learning seminar comes to an end, my heart is full.\nI want to send a big \u0026rsquo;thank you\u0026rsquo; to all of you. Your questions, your intelligence, and your genuine curiosity made every meeting memorable. We dived deep into the world of self-supervised learning, but what I\u0026rsquo;ll cherish the most are the bonds we formed and the shared passion for the scientific world.\nA special shoutout to our each keynote speakers – your presentations and insights added that extra sprinkle of magic to our journey.\nAs we wrap up, my wish for each one of you is simple: Keep that spark alive. Keep questioning. Keep exploring. And remember the good times we had.\nLet\u0026rsquo;s promise to carry forward the warmth, the joy, and the love for learning. And who knows, maybe our paths will cross again in another exciting adventure.\nUntil then, take care and wishing you all the very best.\nBest wishes,\nShengjie.\n The shot for the last seminar personnel\n"},{"id":1,"href":"/23summer/docs/introduction/","title":"Introduction","section":"Docs","content":"Introduction #  Main topics #   Easy-to-understanding Introduction to and advances in some regions of deep learning (Self-supervised learning, meta learning, transfer learning, etc.) - determined by everyone in this seminar. Some practical topics like backbones, optimizers and data augmentations may also incorporated depend on progress.  You can get #    Knowledge about fundamental and advanced methodologies in machine learning.\n  A proof of your study experience: this website - Every contribution you make to this seminar will be recorded\nHope everyone to deeply involved in this seminar, you can\n Serve as a keynote speaker Contribute some blogs and publish on the website    Group Study\n  Work together to sovle for some problems - research experience\n  High-efficent learning pace \u0026amp; Study atmosphere\n    More #   There are no explicit grades in this seminar, the only and foremost objective is to learn together. If you are audience, free free to come or not. (update the topics and main contents in advance). If you are keynote speaker, there are requirements for your representation (discuss later)  "},{"id":2,"href":"/23summer/docs/schedule/","title":"Schedule","section":"Docs","content":"Schedule #     Date Main Topic Speaker     Jun 21 An Introduction of Model-Agnostic Meta-Learning: Principles, Mechanisms, and Variants Wang Ma   Jun 29 The Bridge: Transferable Feature towards Advanced Mechanism Zebing Yun   Jul 5 Semi-Supervised Learning based on Pseudo-labeling Shengjie Niu   Jul 13 The Self-Distillation Family in Self-Supervised Learning Lifan Lin, Yue Wu   Jul 20 The Deep Metric Learning Family in Self-Supervised Learning Xinyao Li, Yiming Zhang, Shengqi Fang   Jul 27 Driving Neural Networks: An Exploration of Optimizers in Deep Learning \u0026amp; Ramblings in Optimization Wang Ma   Aug 3 The Canonical Correlation Analysis Family in Self-Supervised Learning Xunyi Jiang, Langtian Ma   Aug 10 Masked Image Model in Self-Supervised Learning Family in Self-Supervised Learning Shengjie Niu    Please click the topics directly for detailed information like online link, main contents and references.\n Get Home  Mail  "},{"id":3,"href":"/23summer/docs/topics/week1/","title":"Week1","section":"Topics","content":"Week 1 #  Topic: An Introduction of Model-Agnostic Meta-Learning: Principles, Mechanisms, and Variants\nKeynote Speaker: Wang Ma\nTime: Jun 21, 11:00 - 12:30 am\nVenue: Business Hall, 314 (SUSTech)\nOnline Link: TencentMeeting\nCompendium #  I. Introduction: The Concept and Mechanism of Meta-Learning (Credits: Prof. Hung-yi Lee\u0026rsquo;s Slides)\n Discussing the concept and mechanisms of Meta-Learning. Exploring its distinctive role and divergence from traditional Machine Learning approaches.  II. The Emergence of MAML\n Providing an overview of the Model-Agnostic Meta-Learning (MAML) Algorithm. Offering an intricate explanation of MAML\u0026rsquo;s workings, augmented by sketch illustrations. Demonstrating the versatility of MAML through application examples such as classification, regression, and reinforcement learning problems.  III. First-Order MAML (FO-MAML)\n Illuminating the presence of the second derivative in the MAML Algorithm through mathematical derivation. Introducing the First-Order Model-Agnostic Meta-Learning (FO-MAML), with a focus on its divergence from the MAML algorithm (which part of the originial algorithm is ignored). Rewriting the FO-MAML algorithm to better illustrate its structure and functionality.  IV. The Advent of Reptile\n Re-examining the MAML optimization problem and diving deeper into FO-MAML. Introducing Reptile, with a clear delineation of its distinction from FO-MAML. Rewriting the Reptile algorithm for a better understanding.  V. Feature Reuse in MAML \u0026amp; Introduction to ANIL (Almost No Inner Loop)\n Discussing why MAML\u0026rsquo;s efficiency is amplified by Feature Reuse rather than Rapid Learning, supplemented with the explanation and analysis of three experimental case studies. Explaining how the concept of feature reuse spearheaded the idea of ANIL. Providing a comprehensive explanation of ANIL, its unique algorithm, and how it differentiates from MAML.  VI. Conclusion\n Recapping the key concepts, techniques, and distinctions between MAML and its variants, emphasizing their potential impact on the future of machine learning.  Material #  The first week slide from Wang Ma.\n Slide-ref is reference slide from hungyi-lee.\nReferences #    Chelsea Finn\u0026rsquo;s blog on Learning to Learn\n  Chelsea Finn et al. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (MAML)\n  Alex Nichol et al. On First-Order Meta-Learning Algorithms (Reptile)\n  Aniruddh Raghu et al. Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML\n  Chelsea Finn. Learning to Learn with Gradients\n  "},{"id":4,"href":"/23summer/docs/topics/week2/","title":"Week2","section":"Topics","content":"Week 2 #  Topic: The Bridge: Transferable Feature towards Advanced Mechanism\nKeynote Speaker: Zebin Yun\nTime: Jun 29, 19:30 - 21:30 pm\nVenue: Lecture Hall 3, 302 (SUSTech)\nOnline Link: TencentMeeting\nCompendium #  Present transferability of features in deep neural networks, specifically the generality versus specificity of neurons in each layer of a deep convolutional neural network. Furthermore, detail the connection between transferable features and pretrain-finetune mechanism.\n The first layer of a deep neural network learns simple features are generalizable across tasks. As the layers get deeper, the neurons become more specialized to their original task, making them less transferable to new tasks. Fine-tuning a pre-trained network on a new task can be difficult due to optimization difficulties related to splitting the network. The transferability of features can be quantified by measuring the performance of a network when transferring between dis-similar tasks. Networks trained on a natural target task perform better when transferring to a man-made target task than vice versa. 6. The performance of a network can be improved by initializing the first few layers with random, untrained weights. The pretrain weight can utilize the transferable feature for downstream tasks  Material #  I. The second week slide from Zebin Yun.\nII. Some helpful videos:\n  BERT and its family - Introduction and Fine-tune  BERT and its family - ELMo, BERT, GPT, XLNet, MASS, BART, UniLM, ELECTRA, and more  Lecture 21: Auto-encoder (1/2)  Lecture 22: Auto-encoder (2/2)  References #    Yosinski, J et al. How transferable are features in deep neural networks?\n  Devlin, Jacob, et al. Bert: Pre-training of deep bidirectional transformers for language understanding. - Code of BERT\n  S. J. Pan et al. Domain Adaptation via Transfer Component Analysis\n  Long M et al. Transfer feature learning with joint distribution adaptation\n  "},{"id":5,"href":"/23summer/docs/topics/week3/","title":"Week3","section":"Topics","content":"Week 3 #  Topic: Semi-Supervised Learning based on Pseudo-labeling\nKeynote Speaker: Shengjie Niu\nTime: Jul 5, 19:30 - 21:30 pm\nVenue: Business Hall 3, 314 (SUSTech)\nOnline Link: TencentMeeting\nCompendium #  Material #  I. Weekly Slide form Shengjie Niu.\nII. Source code of slide from Overleaf.\nIII. Some tutorials for Semi-supervised Learning:\n  Kihyuk Sohn et al. Fixmatch: Simplifying semi-supervised learning with consistency and confidence..\n   Awesome Semi-supervised Learning, GitHub.\n  Lil\u0026rsquo;s Log, Learning with not Enough Data Part 1: Semi-Supervised Learning.\n  Y Wang et al, USB: A Unified Semi-supervised Learning Benchmark for Classification.\n  References #    Kihyuk Sohn et al. Fixmatch: Simplifying semi-supervised learning with consistency and confidence..\n  Hyuck Lee et al, ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning.\n  Youngtaek Oh et al, DASO: Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning.\n  Zhengfeng Lai et al, Smoothed Adaptive Weighting for Imbalanced Semi-Supervised Learning: Improve Reliability Against Unknown Distribution Data.\n  Qing Yu et al, Multi-Task Curriculum Framework for Open-Set Semi-Supervised Learning.\n  Lan-Zhe Guo et al, Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data.\n  Kuniaki Saito et al, OpenMatch: Open-set Consistency Regularization for Semi-supervised Learning with Outliers.\n  Bowen Zhang et al, FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling.\n  Yidong Wang et al, FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning.\n  Hao Chen et al, SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning.\n  "},{"id":6,"href":"/23summer/docs/topics/week4/","title":"Week4","section":"Topics","content":"Week 4 #  Topic: The Self-Distillation Family in Self-Supervised Learning\nKeynote Speaker: Lifan Lin, Yue Wu, Shengjie Niu\nTime: Jul 13, 19:30 - 21:30 pm\nVenue: Lecture Hall 3, 302 (SUSTech)\nOnline Link: TencentMeeting\nCompendium #  I. Introduction: Concept and Mechanism of Self-Distillation\n Introduce the basic concept and structure of self-distillation. Discuss the mechanisms of self-distillation and its relation with Knowledge distillation. Discuss the basic idea of contrastive learning and its pretext task (SimCLR). The learning objective: A representation mapping invariant of transformation(augmentation). Alignment and uniformity are two key requirement. Collapse(trivial solution). Optimal but unwanted result. Basic concept in preventing collapse.  II. Contrastive Learning without Negative Samples\n Introduce BYOL, a self-distillation model learning without negative pairs. Connections between BYOL and other works Provide some illuminations about why BYOL performs well and avoids collapse  III. Examination of BYOL-like model: Road to prevent collapse\n Mythology: Studying the components of the model through ablation to understand whether they are necessary in preventing collapse. Hypothesis proposed: Procedures taken are actually solving an underlying optimization problem. The optimization is EM-like and thus do well in searching for a representation. Validating the hypothesis vis experience.  IV. Combining Transformer with Self-Distillation\n Discuss the difficulty faced by the ViT(Vision Transformer). Specifically, the tokenizer of images. Tokenizer learn better deep semantic information using self-distillation. Discussing the trade-off between alignment and uniformity in ViT. Improvement: Introduction MIM(Masked Image Modelling, much similar to masked language modelling) to self-distillation to create more effective pretext tasks.  Material #  I. Slide for Intro to Self-Supervised Learning from Shengjie Niu.\nII. Slides 1 and 2 for Self-Distillation Family from Yue Wu and Lifan Lin.\nReferences #    Balestriero, R et al. A Cookbook of Self-supervised Learning\n   Bootstrap your own latent: A new approach to self-supervised Learning\n   Exploring Simple Siamese Representation Learning\n   Emerging Properties in Self-Supervised Vision Transformers\n   A blog about BYOL\n   A Simple Framework for Contrastive Learning of Visual Representations (arxiv.org)\n   Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)\n   Exploring Simple Siamese Representation Learning (arxiv.org)\n   Emerging Properties in Self-Supervised Vision Transformers (arxiv.org)\n   iBOT: Image BERT Pre-Training with Online Tokenizer (arxiv.org)\n   blog_quarto - BYOL: Contrastive Representation-Learning without Contrastive Losses (drscotthawley.github.io)\n   Understanding self-supervised and contrastive learning with \u0026ldquo;Bootstrap Your Own Latent\u0026rdquo; (BYOL) - generally intelligent\n  "},{"id":7,"href":"/23summer/docs/topics/week5/","title":"Week5","section":"Topics","content":"Week 5 #  Topic: The Deep Metric Learning Family in Self-Supervised Learning\nKeynote Speaker: Xinyao Li, Yiming Zhang, Shengqi Fang\nTime: Jul 20, 19:30 - 21:30 pm\nVenue: Lecture Hall 3, 302 (SUSTech)\nOnline Link: TencentMeeting\nCompendium #  I. Contrastive Predictive Coding：\n Introduce Contrastive Predictive Coding, which learns self-supervised representations by predicting the future in latent space by using powerful autoregressive models. Introduce the concept of InfoNCE loss, which is a widely used loss function.  II. The Contrastive Loss Function\n Introduce the core ideas of Deep Metric Learning and discuss the shortcomings of traditional Metric Learning methods. Discuss DrLIM (Dimensionality Reduction by Learning an Invariant Mapping). Introduce Contrastive Loss and its spring model analogy: Attract-only spring and m-Repulse-only spring. Show the experiments of DrLIM along with traditional Metric Learning methods.  III. The Triplet Loss Function\n Introduce Triplet Loss: anchor, positive and negative; easy, hard and semi-hard triplets. Discuss the importance of triplet selection and two methods: choosing semi-hard triplets (applied in FaceNet) and Batch Hard.  VI. A brief review of SimCLR:\n Introduce structure of contrastive learning. emphasize the importance of data augmentation(random cropping and color distortion).  Material #  Slides 1, 2 and 3 for Deep Metric Learning Family from Xinyao Li, Shengqi Fang and Yiming Zhang.\nReference #    Balestriero, R et al. A Cookbook of Self-supervised Learning\n  R Hadsell et al, Dimensionality Reduction by Learning an Invariant Mapping\n  F Schroff et al, FaceNet: A unified embedding for face recognition and clustering\n  A Hermans et al, In Defense of the Triplet Loss for Person Re-Identification\n  J Goldberger et al， Neighbourhood Components Analysis\n  K Sohn et al, Improved Deep Metric Learning with Multi-class N-pair Loss Objective\n  A Oord et al， Representation Learning with Contrastive Predictive Coding\n  T Chen et al, A Simple Framework for Contrastive Learning of Visual Representations\n  A Dosovitskiy et al, Discriminative Unsupervised Feature Learning with Convolutional Neural\n  Z Wu et al, Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination\n  "},{"id":8,"href":"/23summer/docs/topics/week6/","title":"Week6","section":"Topics","content":"Week 6 #  Topic: Driving Neural Networks: An Exploration of Optimizers in Deep Learning \u0026amp; Ramblings in Optimization\nKeynote Speaker: Wang Ma\nTime: Jul 27, 19:30 - 21:00 pm\nVenue: Lecture Hall 3, 302 (SUSTech)\nOnline Link: TencentMeeting\nCompendium #  Update soon\nMaterial #  The weekly slide from Wang Ma.\nReferences #  Update soon\n"},{"id":9,"href":"/23summer/docs/topics/week7/","title":"Week7","section":"Topics","content":"Week 7 #  Topic: The Canonical Correlation Analysis Family in Self-Supervised Learning\nKeynote Speaker: Xunyi Jiang, Langtian Ma\nTime: Aug 3, 19:30 - 21:00 pm\nVenue: Lecture Hall 3, 302 (SUSTech)\nOnline Link: TencentMeeting\nCompendium #  I. Traditional CCA\n Generalized CCA framework Traditional Nonlinear CCA A compressed representatoin approach for CCA Kernel CCA  II. Deep CCA and its variates\n Deep canonical correlation analysis Deep canonically correlated autoencoders  III. CCA in Self-supervised Learning\nThere are 4 major categories of self-supervise methods, including information maximization, clustering, distillation techniques, and contrastive method. In this week, I will introduce 3 methods(W-MSE/Barlow Twins/VICReg) lying in information maximization, and 2 methods(SeLa/SwAV) in clustering.\nThe story line is as following:\n Overview of SSL methods Information maximization methods: W-MSE/Barlow Twins/VICReg Clustering methods: SeLa/SwAV Summary of these methods  Material #  Slides 1 and 2 for Canonical Correlation Analysis Family from Xunyi Jiang and Langtian Ma.\nReferences #    Breiman, L et al, Estimating Optimal Transformations for Multiple Regression and Correlation\n  Painsky A et al, Nonlinear Canonical Correlation Analysis:A Compressed Representation Approach\n  D. R. Hardoon et al, Canonical correlation analysis: An overview with application to learning methods\n  Andrew, G et al Deep canonical correlation analysis\n  Wang, W et al On deep multi-view representation learning\n  A. Ermolov, A. Siarohin, E. Sangineto, and N. Sebe, Whitening for Self-Supervised Representation Learning\n  J. Zbontar, L. Jing, I. Misra, Y. Lecun, and S. Deny, Barlow Twins: Self-Supervised Learning via Redundancy Reduction\n  A. Bardes, J. Ponce, and Y. Lecun, VICREG: VARIANCE-INVARIANCE-COVARIANCE RE- GULARIZATION FOR SELF-SUPERVISED LEARNING\n  Y. Asano, C. Rupprecht, and A. Vedaldi, SELF-LABELLING VIA SIMULTANEOUS CLUSTERING AND REPRESENTATION LEARNING\n  M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\n  "},{"id":10,"href":"/23summer/docs/topics/week8/","title":"Week8","section":"Topics","content":"Week 8 #  Topic: The simple summary of Self-Supervised Learning \u0026amp; Masked Image Model in Self-Supervised Learning Family in Self-Supervised Learning\nKeynote Speaker: Shengjie Niu, Zebin Yun\nTime: Aug 10, 19:30 - 21:30 pm\nVenue: Lecture Hall 3, 302 (SUSTech)\nOnline Link: TencentMeeting\nCompendium #  I. Stage-1: Verstile developments among the field of SSL\n InstDisc proposed new pretext task of instance-level discrimination. CPC employed the predictive pretxt task on SSL, so as to realize wide applicability. InvaSpread utilize the negative pairs among the batch samples. CMC first proposed the utilization from the perspective of mutiview. Stage conclusion in terms of objective functions, pretext tasks and construction of negative pairs.  II. Stage-2: Two distinguished methodologies lead the trend\n MoCo proposed momentum encoder and query dictionary for contrastive Self-Supervised Learning. SimCLR proposed a simple but effective framework to claim the function of augmentation strategies and project head. MoCo v2 and SimCLR v2 SeLa first compared the instance with clustering prototypes instead of negative pairs, and propose a new component of predict head. SwAV achieve the promising performance with sway mode of SeLa, leading to the following trend of learning without negative pairs.  III. Stage-3: Learning without negative pairs\n BYOL: first Self-Supervised Learning literature finished its training totally without negative pairs. SimSiam summarized and analysed the function of all kinds of components, resulting in a structure-easy but effective model.  Significant Timepoint: (2020.10) Vision Transfermer(ViT) brought large impacts on computer vision field and promoted the progress of generative vision model based on ViT.\nIV. Masked Image Model (MIM) family in Self-Supervised Learning\n BEiT trained transformer-based model to reconstruct the input in the form of token-level label (In need of pre-trained tokenizer) MAE trained transformer-based model to reconstercut the input in pixel level SimMIM summarized some MIM models and found a series of simple but effective components.  Material #   Slides \u0026amp; Source Code for Week-8 seminar from Shengjie Niu.\n Big Picture from Shengjie Niu.\nReference #    Z. Wu et al, Unsupervised Feature Learning via Non-Parametric Instance Discrimination.\n  A. Oord et al, Representation Learning with Contrastive Predictive Coding.\n  M. Ye et al, Unsupervised Embedding Learning via Invariant and Spreading Instance Feature.\n  Y. Tian et al, Contrastive Multiview Coding.\n  K. He et al, Momentum Contrast for Unsupervised Visual Representation Learning.\n  T. Chen et al, A Simple Framework for Contrastive Learning of Visual Representations.\n  X. Chen et al, Improved Baselines with Momentum Contrastive Learning.\n  T. Chen et al, Big Self-Supervised Models are Strong Semi-Supervised Learners.\n  Y. Asano et al, Self-labelling via simultaneous clustering and representation learning.\n  M. Caron et al, Unsupervised Learning of Visual Features by Contrasting Cluster Assignments.\n  J. Grill et al, Bootstrap your own latent: A new approach to self-supervised Learning.\n  X. Chen et al, Exploring Simple Siamese Representation Learning.\n  M. Caron et al, Emerging Properties in Self-Supervised Vision Transformers.\n  B. Hao et al, BEiT: BERT Pre-Training of Image Transformers.\n  K He et al, Masked Autoencoders Are Scalable Vision Learners.\n  Z Xie et al, SimMIM: A Simple Framework for Masked Image Modeling.\n  "}]